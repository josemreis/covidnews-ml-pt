{"headlines":{"0":"Rob\u00f4s assassinos. Queremos ir para o lado negro da for\u00e7a?"},"author":{"0":"Joana Marques Alves"},"pub_date":{"0":"18 September 2019"},"source_name":{"0":"Jornal I"},"word_count":{"0":"803 words"},"language":{"0":"Portuguese"},"leading_paragraph":{"0":"Os avan\u00e7os tecnol\u00f3gicos n\u00e3o trazem s\u00f3 coisas boas: uma ex-funcion\u00e1ria da Google alertou para o perigo iminente que as armas aut\u00f3nomas letais representam. Alguma vez pensou viver numa era em que existem rob\u00f4s assassinos? Sim, leu bem, 'rob\u00f4s assassinos'. A express\u00e3o \u00e9 usada por Laura Nolan, uma antiga funcion\u00e1ria da Google, que est\u00e1 a tentar alertar as Na\u00e7\u00f5es Unidas para os perigos da maquinaria que est\u00e1 a ser criada por empresas tecnol\u00f3gicas. Bem-vindo ao (lado negro) do s\u00e9culo XXI.\nNolan, que se despediu da Google no ano passado em protesto por ter sido colocada num projeto que visava desenvolver tecnologia militar para os Estados Unidos, disse ao jornal Guardian que a nova gera\u00e7\u00e3o de armas aut\u00f3nomas que est\u00e1 a ser criada pode acidentalmente desencadear uma guerra ou atrocidades em massa. A engenheira inform\u00e1tica defende que estas armas devem ser banidas e controladas da mesma forma que as armas qu\u00edmicas."},"remaining_content":{"0":"Ao contr\u00e1rio dos drones, que s\u00e3o controlados por militares, os rob\u00f4s aut\u00f3nomos letais conseguem detetar, identificar, selecionar e atacar alvos sem que haja a interven\u00e7\u00e3o de seres humanos. Nolan defende que estes 'rob\u00f4s assassinos t\u00eam potencial para criar situa\u00e7\u00f5es calamitosas para as quais n\u00e3o foram inicialmente programados'.\nSem apontar o dedo a qualquer gigante tecnol\u00f3gico, Nolan alerta para os perigos do que est\u00e1 atualmente a ser desenvolvido. 'A probabilidade de ocorrer um desastre \u00e9 proporcional \u00e0 quantidade de m\u00e1quinas que est\u00e3o ao mesmo tempo no local. Falamos de poss\u00edveis atrocidades e de assass\u00ednios ilegais cometidos mesmo sob as leis da guerra', explica a engenheira, citada pelo Guardian.\n'Os rob\u00f4s poder\u00e3o come\u00e7ar a comportar-se de uma forma diferente do que era expect\u00e1vel. Por isso mesmo, quaisquer sistemas avan\u00e7ados de armamento deveriam estar sujeitos a um controlo humano. Caso contr\u00e1rio, devem ser banidos. S\u00e3o demasiado imprevis\u00edveis e perigosos', defende Laura Nolan, que j\u00e1 esteve reunida com representantes das Na\u00e7\u00f5es Unidas para expor as suas preocupa\u00e7\u00f5es em rela\u00e7\u00e3o a este problema.\nAl\u00e9m disso, existem fatores externos que podem influenciar o comportamento do rob\u00f4: 'Pode, por exemplo, encontrar um grupo de homens armados que \u00e9 identificado como sendo o inimigo, quando na verdade s\u00e3o apenas sobreviventes \u00e0 procura de comida. As m\u00e1quinas n\u00e3o t\u00eam o discernimento ou o senso comum que os humanos possuem'.\n'Outro aspeto assustador dos sistemas de guerra aut\u00f3nomos \u00e9 o facto de s\u00f3 poderem ser testados numa zona de combate real. Se calhar isso est\u00e1 a acontecer agora com os russos na S\u00edria, quem sabe? O que sabemos \u00e9 que, nas Na\u00e7\u00f5es Unidas, a R\u00fassia n\u00e3o quis assinar qualquer tratado [para travar estes rob\u00f4s], quanto mais bani-los', afirma Laura Nolan. 'Se est\u00e1s a testar uma m\u00e1quina que toma as suas pr\u00f3prias decis\u00f5es, ent\u00e3o tens de a testar no mundo real. Al\u00e9m disso, como se treina um software a detetar mudan\u00e7as muito subtis no comportamento humano ou a distinguir a diferente entre um ca\u00e7ador e um insurgente? Com \u00e9 que um rob\u00f4 destes consegue, ao sobrevoar uma \u00e1rea, perceber a diferen\u00e7a entre um combatente de 18 anos e um ca\u00e7ador de coelhos de 18 anos?', questiona.\nNo entanto, a engenheira inform\u00e1tica garante que n\u00e3o quer ser \u2018mais papista que o Papa\u2019: 'N\u00e3o quero com isto dizer que os sistemas de orienta\u00e7\u00e3o de m\u00edsseis ou de defesa antim\u00edssil devem ser banidos. Estes s\u00e3o, apesar de tudo, controlados pelo Homem e h\u00e1 sempre algu\u00e9m que \u00e9 respons\u00e1vel pelo que acontece. Mas h\u00e1 muito poucas pessoas a falar sobre os perigos das armas aut\u00f3nomas e se, n\u00e3o tivermos cuidado, em menos de nada, um destes rob\u00f4s assassinos poder\u00e1 cometer uma atrocidade' .\nQuest\u00e3o \u00e9tica Nolan come\u00e7ou a trabalhar para a Google em 2013. Quatro anos depois, em 2017, foi chamada para se juntar ao Project Maven, que tinha como objetivo ajudar o departamento de Defesa norte-americano melhorando os drones militares e a sua capacidade de reconhecer pessoas e objetos. A engenheira inform\u00e1tica explica que come\u00e7ou a ter 'cada vez mais preocupa\u00e7\u00f5es com quest\u00f5es \u00e9ticas' \u00e0 medida que o programa ia sendo desenvolvido.\n'Apesar de n\u00e3o estar diretamente envolvida nos trabalhos para acelerar os mecanismos de reconhecimento, percebi que fazia parte da cadeia e que isto acabaria por colocar mais vidas em causa em s\u00edtios como o Afeganist\u00e3o', acrescenta. A verdade \u00e9 que a Google acabou mesmo por deixar cair o Project Maven, depois de mais de tr\u00eas mil empregados terem assinado uma peti\u00e7\u00e3o a exigir que a empresa se desassociasse desta iniciativa.\nN\u00e3o existe qualquer ind\u00edcio de que a Google esteja a trabalhar em armas aut\u00f3nomas letais. Ali\u00e1s, segundo o Guardian, no m\u00eas passado, um grupo de especialistas das Na\u00e7\u00f5es Unidas discutiram este tema e chegaram \u00e0 conclus\u00e3o de que a Google n\u00e3o estava envolvida neste mercado, utilizando a Intelig\u00eancia Artificial desenvolvida apenas em meios seguros.\nclique para ver a imagem"},"query":{"0":""},"query_date_from":{"0":"2019\/09\/18"},"query_date_to":{"0":"2019\/09\/18"},"doc_id":{"0":"JORNPT0020190918ef9i0012y"},"has_corona_label":{"0":false}}